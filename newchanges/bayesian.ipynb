{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4370fb2d",
   "metadata": {},
   "source": [
    "# Bayesian Network Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "Two Bayesian Network classifiers were developed: Gaussian Naive Bayes (GNB) using continuous features and Categorical Naive Bayes (CNB) using discretized features. Both models were trained on the same 12,076 patient dataset with 70/30 train-test split.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Performance Comparison\n",
    "\n",
    "### 2.1 Gaussian vs. Categorical Naive Bayes\n",
    "\n",
    "| Metric | Gaussian NB | Categorical NB | Winner |\n",
    "|--------|-------------|----------------|--------|\n",
    "| **Accuracy** | **93.18%** | 92.16% | Gaussian (+1.02%) |\n",
    "| **Precision** | **93.73%** | 91.18% | Gaussian (+2.55%) |\n",
    "| **Recall** | **86.70%** | 86.39% | Gaussian (+0.31%) |\n",
    "| **F1-Score** | **90.08%** | 88.72% | Gaussian (+1.36%) |\n",
    "| **ROC-AUC** | 0.9372 | **0.9413** | Categorical (+0.41%) |\n",
    "| **Specificity** | **96.78%** | 95.36% | Gaussian (+1.42%) |\n",
    "| **CV Mean F1** | **0.9010** | 0.8802 | Gaussian (+2.08%) |\n",
    "\n",
    "### 2.2 Key Findings\n",
    "\n",
    "**Gaussian NB is the superior model:**\n",
    "- Higher accuracy (+1.02%), precision (+2.55%), and F1-score (+1.36%)\n",
    "- Excellent specificity (96.78%) - best at identifying healthy children\n",
    "- More stable cross-validation performance (CV F1: 0.9010 vs 0.8802)\n",
    "- Slightly better sensitivity (86.70% vs 86.39%)\n",
    "\n",
    "**Categorical NB advantage:**\n",
    "- Marginally higher ROC-AUC (0.9413 vs 0.9372) despite lower accuracy\n",
    "\n",
    "Gaussian Naive Bayes is recommended for this application due to consistently better performance across most metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Confusion Matrix Analysis\n",
    "\n",
    "### 3.1 Gaussian Naive Bayes (Test Set, N=3,623)\n",
    "\n",
    "|  | **Predicted Normal** | **Predicted Anormal** | **Total** |\n",
    "|---|---|---|---|\n",
    "| **Actual Normal** | 2,255 (TN) | 75 (FP) | 2,330 |\n",
    "| **Actual Anormal** | 172 (FN) | 1,121 (TP) | 1,293 |\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Sensitivity: 86.70%** - Correctly identifies 1,121 of 1,293 pathological cases\n",
    "- **Specificity: 96.78%** - Correctly identifies 2,255 of 2,330 healthy children\n",
    "- **Precision: 93.73%** - When predicting \"Anormal,\" correct 93.73% of time\n",
    "- **False Negatives: 172 (13.30%)** - Most critical error (missed pathologies)\n",
    "- **False Positives: 75 (3.22%)** - Minimal false alarms\n",
    "\n",
    "### 3.2 Categorical Naive Bayes (Test Set, N=3,623)\n",
    "\n",
    "|  | **Predicted Normal** | **Predicted Anormal** | **Total** |\n",
    "|---|---|---|---|\n",
    "| **Actual Normal** | 2,222 (TN) | 108 (FP) | 2,330 |\n",
    "| **Actual Anormal** | 176 (FN) | 1,117 (TP) | 1,293 |\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Sensitivity: 86.39%** - Slightly lower than Gaussian NB\n",
    "- **Specificity: 95.36%** - Lower specificity (more false positives: 108 vs 75)\n",
    "- **Precision: 91.18%** - Lower precision due to more false alarms\n",
    "- **False Negatives: 176 (13.61%)** - Slightly more missed cases than Gaussian\n",
    "\n",
    "### 3.3 Clinical Interpretation\n",
    "\n",
    "**Gaussian NB advantages:**\n",
    "1. **Fewer false positives (75 vs 108)** → Reduces unnecessary follow-up tests for 33 additional healthy children\n",
    "2. **Fewer false negatives (172 vs 176)** → Catches 4 more pathological cases\n",
    "3. **Higher specificity (96.78%)** → Better at ruling out disease in healthy children\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Importance Analysis\n",
    "\n",
    "### 4.1 Discriminative Power (Gaussian NB)\n",
    "\n",
    "Based on mean differences between Normal and Anormal classes:\n",
    "\n",
    "| Rank | Feature | Normalized Importance | Mean Diff | Clinical Interpretation |\n",
    "|------|---------|---------------------|-----------|------------------------|\n",
    "| 1 | FC | 59.23% | 3.815 | Heart rate differs most between classes (Anormal: 89.4 vs Normal: 85.6 bpm) |\n",
    "| 2 | SOPRO** | 25.45% | 1.639 | Heart murmur presence strongly discriminates (encoded: Anormal: 0.32 vs Normal: 1.95) |\n",
    "| 3 | IMC | 7.15% | 0.460 | Minor BMI difference (Anormal: 17.57 vs Normal: 18.03) |\n",
    "| 4 | MOTIVO2 | 4.29% | 0.276 | Secondary referral reason provides minimal information |\n",
    "| 5 | B2 | 2.18% | 0.140 | Second heart sound shows small difference |\n",
    "| 6 | MOTIVO1 | 0.91% | 0.059 | Primary referral reason nearly irrelevant |\n",
    "| 7 | PA_SISTOLICA | 0.79% | 0.051 | Systolic BP virtually identical between classes |\n",
    "\n",
    "### 4.2 Critical Insights\n",
    "\n",
    "**1. FC (Heart Rate) dominates in Bayesian Networks (59.23%)**\n",
    "- Contrasts sharply with Decision Tree where SOPRO was 92.26%\n",
    "- Anormal patients have ~3.8 bpm higher heart rate on average\n",
    "- This reflects the Gaussian NB's sensitivity to continuous variable distributions\n",
    "\n",
    "**2. SOPRO remains highly important (25.45%)**\n",
    "- Second most discriminative feature\n",
    "- Mean encoding difference: Anormal patients score much lower (0.32 vs 1.95)\n",
    "- Confirms multivariate analysis and Decision Tree findings\n",
    "\n",
    "**3. Other features contribute minimally (<8% combined)**\n",
    "- IMC, MOTIVO1, MOTIVO2, B2, PA_SISTOLICA have limited discriminative power\n",
    "- Suggests a simpler 2-feature model (FC + SOPRO) might perform nearly as well\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ROC Curve Analysis\n",
    "\n",
    "### 5.1 AUC Scores\n",
    "\n",
    "- **Gaussian NB:** AUC = 0.9372\n",
    "- **Categorical NB:** AUC = 0.9413\n",
    "- **Difference:** +0.0041 in favor of Categorical NB\n",
    "\n",
    "### 5.2 Interpretation\n",
    "\n",
    "**Both models achieve excellent discrimination (>0.93 AUC):**\n",
    "- 93-94% probability that a randomly selected Anormal patient receives a higher risk score than a randomly selected Normal patient\n",
    "- Both far exceed random classifier (AUC = 0.50)\n",
    "- Minimal practical difference between the two (0.4% AUC difference)\n",
    "\n",
    "**Categorical NB's higher AUC despite lower accuracy suggests:**\n",
    "- Better probability calibration across different decision thresholds\n",
    "- May produce more reliable probability estimates for risk stratification\n",
    "- Could be valuable in clinical settings where uncertainty quantification matters\n",
    "\n",
    "**Clinical implication:**\n",
    "- Both models suitable for risk scoring systems\n",
    "- Categorical NB slightly better for generating confidence scores\n",
    "- Gaussian NB better for binary classification (yes/no decisions)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Probability Distribution Analysis\n",
    "\n",
    "### 6.1 Key Observations\n",
    "\n",
    "The probability distribution plots reveal:\n",
    "\n",
    "**Gaussian NB:**\n",
    "- **Normal cases:** Most predicted probabilities cluster near 0 (low Anormal probability)\n",
    "- **Anormal cases:** Most predicted probabilities cluster near 1 (high Anormal probability)\n",
    "- **Good separation:** Clear bimodal distribution indicates confident predictions\n",
    "- **Few borderline cases:** Limited overlap around 0.5 threshold\n",
    "\n",
    "**Categorical NB:**\n",
    "- **Similar pattern** but slightly more overlap around decision boundary\n",
    "- **More conservative predictions:** Less extreme probabilities (fewer 0.0 or 1.0 predictions)\n",
    "- **Wider distributions:** More uncertainty, which explains higher AUC with lower accuracy\n",
    "\n",
    "### 6.2 Clinical Utility\n",
    "\n",
    "Well-separated probability distributions mean:\n",
    "- High confidence in most predictions\n",
    "- Clear identification of borderline cases (probabilities near 0.5)\n",
    "- Clinicians can stratify patients by risk level, not just binary classification\n",
    "- Patients with P(Anormal) = 0.7-0.9 might need different follow-up than those with P(Anormal) > 0.95\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Cross-Validation Results\n",
    "\n",
    "### 7.1 Gaussian Naive Bayes\n",
    "\n",
    "| Fold | F1-Score |\n",
    "|------|----------|\n",
    "| 1 | 0.8912 |\n",
    "| 2 | 0.9055 |\n",
    "| 3 | 0.9063 |\n",
    "| 4 | 0.9040 |\n",
    "| 5 | 0.8981 |\n",
    "\n",
    "**Mean F1:** 0.9010 ± 0.0057  \n",
    "**95% CI:** [0.8899, 0.9121]\n",
    "\n",
    "### 7.2 Categorical Naive Bayes\n",
    "\n",
    "| Fold | F1-Score |\n",
    "|------|----------|\n",
    "| 1 | 0.8724 |\n",
    "| 2 | 0.8824 |\n",
    "| 3 | 0.8874 |\n",
    "| 4 | 0.8838 |\n",
    "| 5 | 0.8753 |\n",
    "\n",
    "**Mean F1:** 0.8802 ± 0.0056  \n",
    "**95% CI:** [0.8694, 0.8911]\n",
    "\n",
    "### 7.3 Stability Analysis\n",
    "\n",
    "**Both models show excellent stability:**\n",
    "- **Low standard deviations** (<0.6%) indicate consistent performance across folds\n",
    "- **Narrow confidence intervals** suggest reliable generalization\n",
    "- **Gaussian NB more stable:** Slightly tighter CV range (0.8912-0.9063 vs 0.8724-0.8874)\n",
    "\n",
    "**Gaussian vs Test Performance:**\n",
    "- CV mean (0.9010) ≈ Test F1 (0.9008) → Minimal overfitting\n",
    "- Confirms model generalizes well to unseen data\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Independence Assumption Analysis\n",
    "\n",
    "### 8.1 Correlation Matrix (Numeric Features)\n",
    "\n",
    "|  | FC | IMC | PA_SISTOLICA |\n",
    "|---|---|---|---|\n",
    "| **FC** | 1.000 | -0.139 | -0.111 |\n",
    "| **IMC** | -0.139 | 1.000 | **0.416** |\n",
    "| **PA_SISTOLICA** | -0.111 | **0.416** | 1.000 |\n",
    "\n",
    "### 8.2 Violation Assessment\n",
    "\n",
    "**Detected violation:**\n",
    "- **IMC ↔ PA_SISTOLICA: r = 0.416** (moderate positive correlation)\n",
    "- Violates the Naive Bayes independence assumption\n",
    "\n",
    "**Other correlations:**\n",
    "- FC ↔ IMC: -0.139 (weak, acceptable)\n",
    "- FC ↔ PA_SISTOLICA: -0.111 (weak, acceptable)\n",
    "\n",
    "### 8.3 Impact on Model Performance\n",
    "\n",
    "**Despite the IMC-PA_SISTOLICA correlation violation:**\n",
    "- Model still achieves 93.18% accuracy\n",
    "- Cross-validation shows stability\n",
    "- Suggests the correlation doesn't severely impact performance\n",
    "\n",
    "**Why it still works:**\n",
    "1. **Weak violations:** Only one pair exceeds 0.3 threshold, and even that is moderate (0.416)\n",
    "2. **SOPRO and FC dominate:** The most important features (59% + 25% = 84% importance) are uncorrelated\n",
    "3. **Naive Bayes robustness:** Known to work well even with mild independence violations\n",
    "4. **Within-class independence:** The assumption is conditional independence *given the class*, which may hold better than marginal independence\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 9. Model Strengths & Limitations\n",
    "\n",
    "### 9.1 Gaussian Naive Bayes Strengths\n",
    "\n",
    "1. **Excellent performance:** 93.18% accuracy, 0.9372 AUC\n",
    "2. **High specificity:** 96.78% - minimizes false alarms\n",
    "3. **Probabilistic predictions:** Provides P(Anormal) for risk stratification\n",
    "4. **Fast & scalable:** Trains instantly, predicts in real-time\n",
    "5. **Robust to violations:** Works well despite IMC-PA_SISTOLICA correlation\n",
    "6. **Simple & stable:** No hyperparameter tuning needed, low CV variance\n",
    "7. **Handles missing data:** Naturally accommodates incomplete records\n",
    "\n",
    "### 9.2 Limitations\n",
    "\n",
    "1. **Independence assumption:** Assumes features are conditionally independent (violated by IMC-PA_SISTOLICA)\n",
    "2. **Normality assumption:** Assumes Gaussian distributions (may not hold for all features)\n",
    "3. **Lower interpretability:** Cannot visualize decision rules like trees\n",
    "4. **Feature importance paradox:** FC ranks #1 but SOPRO clinically more important\n",
    "5. **False negatives:** Still misses 13.3% of pathological cases (172 patients)\n",
    "6. **Sensitive to feature scaling:** Different scales can affect probability estimates\n",
    "\n",
    "### 9.3 Clinical Risk Assessment\n",
    "\n",
    "**13.3% false negative rate (172 missed cases) considerations:**\n",
    "- **Acceptable for screening:** High sensitivity (86.7%) catches most cases\n",
    "- **Not sufficient for diagnosis:** Requires confirmatory testing (echocardiogram)\n",
    "- **Age/risk stratification:** May perform better/worse in specific age groups\n",
    "- **Clinical context matters:** Symptoms, family history should override model when present\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "### 10.1 Main Findings\n",
    "\n",
    "1. **Gaussian Naive Bayes achieves 93.18% accuracy**\n",
    "\n",
    "2. **FC (heart rate) is the most discriminative feature** in probabilistic models (59.23% importance), contrasting with Decision Tree's SOPRO dominance (92.26%)\n",
    "\n",
    "3. **Both Bayesian models exceed 0.93 AUC**, demonstrating excellent discrimination despite the independence assumption violation\n",
    "\n",
    "4. **Specificity of 96.78%** means only 3.22% of healthy children are incorrectly flagged, minimizing unnecessary testing\n",
    "\n",
    "5. **Categorical NB's higher AUC (0.9413) with lower accuracy** suggests better probability calibration for risk stratification applications\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Conclusions\n",
    "\n",
    "The Bayesian Network analysis demonstrates that Gaussian Naive Bayes is a highly effective, simple, and fast for predicting cardiac pathology in children. With 93.18% accuracy, 96.78% specificity, and 0.9372 AUC.\n",
    "\n",
    "The analysis reveals that heart rate (FC) and heart murmur (SOPRO) together account for 84.68% of discriminative power, suggesting these two clinical features are sufficient for accurate prediction. The model's probabilistic output enables risk stratification beyond binary classification, adding clinical utility for triaging patients.\n",
    "\n",
    "Despite violating the independence assumption (IMC-PA_SISTOLICA correlation of 0.416), the model maintains excellent performance and stability (CV std = 0.0057), demonstrating Naive Bayes's practical robustness.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
